{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######\n",
    "\n",
    "使用续费率的数据实现了wide & deep模型    \n",
    "使用的api为contrib,可能面临被频繁替换的危险， 因此在tf_WD_core中重新改写api\n",
    "\n",
    "\n",
    "结果：预测续费的的pr_auc达到0.938，但是没有解释性\n",
    "\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "df_all = pd.read_csv('data/all.csv').drop(['Unnamed: 0'], axis=1)\n",
    "df_part = pd.concat([df_all.iloc[:, :23], df_all.iloc[:, 71:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c3931fe50>, '_model_dir': '/var/folders/p2/4dg2st2j5dq310jd40nwwcdw0000gn/T/tmpt1CpLZ', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "# categorical base column(5)——离散类别列\n",
    "visa_typ_cd = tf.contrib.layers.sparse_column_with_keys(\n",
    "    column_name=\"visa_typ_cd\", \n",
    "    keys=[\"A\", \"A1\", \"A2\", \"A3\", \"B\", \"C\", \"D1\", \"D2\", \"D3\"])\n",
    "top_schl_id = tf.contrib.layers.sparse_column_with_keys(\n",
    "    column_name=\"top_schl_id\", \n",
    "    keys=[\"432466\", \"430552\", \"431016\", \"432643\", \"434410\",\n",
    "       \"432715\", \"434200\", \"409\", \"432710\", \"430564\",\n",
    "       \"432699\", \"436223\", \"434266\", \"437282\", \"432468\",\n",
    "       \"439174\", \"435391\", \"432590\", \"432791\", \"437357\",\n",
    "       \"438276\", \"436224\", \"435620\", \"430558\", \"432610\",\n",
    "       \"439177\", \"437353\", \"438456\", \"430627\", \"439175\",\n",
    "       \"437317\", \"437367\", \"438445\", \"435300\", \"436070\",\n",
    "       \"437414\", \"435590\", \"439176\", \"435467\", \"440467\"])\n",
    "year = tf.contrib.layers.sparse_column_with_keys(\n",
    "    column_name=\"year\", \n",
    "    keys=[\"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\"])\n",
    "city = tf.contrib.layers.sparse_column_with_keys(\n",
    "    column_name=\"city\", \n",
    "    keys=[\"changchun\", \"shanghai\", \"beijin\", \"zhuhai\", \"yantai\", \"changsha\",\n",
    "        \"chongqing\", \"yangzhou\", \"linfen\", \"taizhou\", \"wenzhou\", \"wuhan\",\n",
    "        \"jiamusi\", \"nantong\", \"wuxi\", \"hangzhou\", \"dalian\", \"huzhou\",\n",
    "        \"shenzheng\", \"xian\", \"ningbo\", \"changzhou\"])\n",
    "level = tf.contrib.layers.sparse_column_with_keys(\n",
    "    column_name=\"level\", \n",
    "    keys=[\"2nd\", \"1st\", \"new1\", \"4th\", \"3rd\"])\n",
    "\n",
    "\n",
    "\n",
    "# continues column(27)——数值列\n",
    "par_days = tf.contrib.layers.real_valued_column(\"par_days\")\n",
    "actual_day = tf.contrib.layers.real_valued_column(\"actual_day\")\n",
    "sub_tol = tf.contrib.layers.real_valued_column(\"sub_tol\")\n",
    "sub_avg = tf.contrib.layers.real_valued_column(\"sub_avg\")\n",
    "sub_avg_a = tf.contrib.layers.real_valued_column(\"sub_avg_a\")\n",
    "ex_tol = tf.contrib.layers.real_valued_column(\"ex_tol\")\n",
    "ex_subrate = tf.contrib.layers.real_valued_column(\"ex_subrate\")\n",
    "sub_ins_sd = tf.contrib.layers.real_valued_column(\"sub_ins_sd\")\n",
    "sub_ins_m = tf.contrib.layers.real_valued_column(\"sub_ins_m\")\n",
    "ins_m = tf.contrib.layers.real_valued_column(\"ins_m\")\n",
    "auc_sub_avg = tf.contrib.layers.real_valued_column(\"auc_sub_avg\")\n",
    "auc_sub_sd = tf.contrib.layers.real_valued_column(\"auc_sub_sd\")\n",
    "ins_max = tf.contrib.layers.real_valued_column(\"ins_max\")\n",
    "ins_summit_four_1 = tf.contrib.layers.real_valued_column(\"ins_summit_four_1\")\n",
    "ins_summit_four_2 = tf.contrib.layers.real_valued_column(\"ins_summit_four_2\")\n",
    "ins_summit_four_3 = tf.contrib.layers.real_valued_column(\"ins_summit_four_3\")\n",
    "ins_summit_max = tf.contrib.layers.real_valued_column(\"ins_summit_max\")\n",
    "notscWe_sub = tf.contrib.layers.real_valued_column(\"notscWe_sub\")\n",
    "notscWe_ex = tf.contrib.layers.real_valued_column(\"notscWe_ex\")\n",
    "notscWd_sub = tf.contrib.layers.real_valued_column(\"notscWd_sub\")\n",
    "notscWd_ex = tf.contrib.layers.real_valued_column(\"notscWd_ex\")\n",
    "sc_sub = tf.contrib.layers.real_valued_column(\"sc_sub\")\n",
    "sc_ex = tf.contrib.layers.real_valued_column(\"sc_ex\")\n",
    "pub_avg = tf.contrib.layers.real_valued_column(\"pub_avg\")\n",
    "pub_sd = tf.contrib.layers.real_valued_column(\"pub_sd\")\n",
    "n_ex = tf.contrib.layers.real_valued_column(\"n_ex\")\n",
    "n_pkg = tf.contrib.layers.real_valued_column(\"n_pkg\")\n",
    "n_pra_ex_avg = tf.contrib.layers.real_valued_column(\"n_pra_ex_avg\")\n",
    "n_pra_ex_sd = tf.contrib.layers.real_valued_column(\"n_pra_ex_sd\")\n",
    "delta_days = tf.contrib.layers.real_valued_column(\"delta_days\")\n",
    "\n",
    "\n",
    "# bucketized categorical column(3)——分桶列\n",
    "par_days_buckets = tf.contrib.layers.bucketized_column(\n",
    "    par_days, \n",
    "    boundaries=[7, 31, 93, 183, 365, 730])\n",
    "actual_day_buckets = tf.contrib.layers.bucketized_column(\n",
    "    actual_day, \n",
    "    boundaries=[5, 25, 75, 150, 300, 600])\n",
    "delta_days_buckets = tf.contrib.layers.bucketized_column(\n",
    "    delta_days, \n",
    "    boundaries=[7, 31, 93, 183, 365, 730, 1650])\n",
    "\n",
    "# define wide features\n",
    "wide_columns = [\n",
    "    visa_typ_cd, \n",
    "    city,\n",
    "    top_schl_id, year, level, par_days_buckets, \n",
    "    actual_day_buckets, delta_days_buckets,\n",
    "    tf.contrib.layers.crossed_column([visa_typ_cd, top_schl_id], hash_bucket_size=int(1e4)),\n",
    "    tf.contrib.layers.crossed_column([visa_typ_cd, year], hash_bucket_size=int(1e4)),\n",
    "    tf.contrib.layers.crossed_column([visa_typ_cd, city], hash_bucket_size=int(1e4)),\n",
    "    tf.contrib.layers.crossed_column([visa_typ_cd, level], hash_bucket_size=int(1e4)),\n",
    "    tf.contrib.layers.crossed_column([visa_typ_cd, top_schl_id, year], hash_bucket_size=int(1e4)),\n",
    "    tf.contrib.layers.crossed_column([year, top_schl_id], hash_bucket_size=int(1e4))    \n",
    "]\n",
    "\n",
    "# define deep features\n",
    "# embedding的维度越高，这个模型不得不学习的特征表示的自由度越大。基于经验，一般选择log2(n) 【以2为底】,n是一个特征列中唯一特征的数量\n",
    "deep_columns = [\n",
    "   par_days, actual_day, sub_tol, sub_avg, sub_avg_a, ex_tol, ex_subrate, sub_ins_sd, \n",
    "   sub_ins_m, ins_m, auc_sub_avg, auc_sub_sd, ins_max, ins_summit_four_1, ins_summit_four_2,\n",
    "   ins_summit_four_3, ins_summit_max, notscWe_sub, notscWe_ex, notscWd_sub, notscWd_ex, \n",
    "   sc_sub, sc_ex, pub_avg, pub_sd, n_ex, n_pkg, n_pra_ex_avg, n_pra_ex_sd, delta_days,\n",
    "   tf.contrib.layers.embedding_column(visa_typ_cd, dimension=3),\n",
    "   tf.contrib.layers.embedding_column(top_schl_id, dimension=5),\n",
    "   tf.contrib.layers.embedding_column(year, dimension=3),\n",
    "   tf.contrib.layers.embedding_column(city, dimension=5),\n",
    "   tf.contrib.layers.embedding_column(level, dimension=3)\n",
    "]\n",
    "\n",
    "# 创建临时文件存储过程数据\n",
    "model_dir = tempfile.mkdtemp() # create a temp path\n",
    "m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir, \n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100,60,30]   \n",
    ")\n",
    "\n",
    "\n",
    "# define the column names for datasets\n",
    "COLUMNS = [\n",
    "        'visa_typ_cd', \n",
    "        'city'\n",
    "        'par_days', 'actual_day', 'sub_tol',\n",
    "        'sub_avg', 'sub_avg_a', 'ex_tol', 'ex_subrate', 'sub_ins_sd',\n",
    "        'sub_ins_m', 'ins_m', 'auc_sub_avg', 'auc_sub_sd', 'ins_max',\n",
    "        'ins_summit_four_1', 'ins_summit_four_2', 'ins_summit_four_3',\n",
    "        'ins_summit_max', 'notscWe_sub', 'notscWe_ex', 'notscWd_sub',\n",
    "        'notscWd_ex', 'sc_sub', 'sc_ex', 'pub_avg', 'pub_sd', 'n_ex',\n",
    "        'n_pkg', 'n_pra_ex_avg', 'n_pra_ex_sd', 'top_schl_id',\n",
    "        'year', 'level','delta_days']\n",
    "\n",
    "LABEL_COLUMN = 'y'\n",
    "\n",
    "CATEGORY_COLUMNS = [\n",
    "    'visa_typ_cd',\n",
    "    'city',\n",
    "    'top_schl_id','year', 'level']\n",
    "\n",
    "CONTINUES_COLUMNS = ['par_days', 'actual_day', 'sub_tol',\n",
    "       'sub_avg','sub_avg_a', 'ex_tol', 'ex_subrate', 'sub_ins_sd',\n",
    "       'sub_ins_m', 'ins_m', 'auc_sub_avg', 'auc_sub_sd', 'ins_max',\n",
    "       'ins_summit_four_1', 'ins_summit_four_2', 'ins_summit_four_3',\n",
    "       'ins_summit_max', 'notscWe_sub', 'notscWe_ex', 'notscWd_sub',\n",
    "       'notscWd_ex', 'sc_sub', 'sc_ex', 'pub_avg', 'pub_sd', 'n_ex',\n",
    "       'n_pkg', 'n_pra_ex_avg', 'n_pra_ex_sd','delta_days']\n",
    "\n",
    "\n",
    "# shuffle the dataset\n",
    "df_test = df_part.sample(frac=0.008)\n",
    "# choose the test dataset in the left & shuffle\n",
    "df_train = df_part[[False if i in df_test.index else True for i in range(len(df_part))]].sample(frac=0.024)\n",
    "\n",
    "test_label = df_part[[True if i in df_test.index.values else False for i in df_part.index.values]].y.values\n",
    "test_label = [int(label) for label in test_label]\n",
    "train_label = df_part[[True if i in df_train.index.values else False for i in df_part.index.values]].y.values\n",
    "train_label = [int(label) for label in train_label]\n",
    "df_test[LABEL_COLUMN] = test_label\n",
    "df_train[LABEL_COLUMN] = train_label\n",
    "df_train = df_train.reset_index().drop(['index'], axis=1)\n",
    "df_test = df_test.reset_index().drop(['index'], axis=1)\n",
    "df_train['top_schl_id'] = [str(val) for val in df_train.top_schl_id.values]\n",
    "df_test['top_schl_id'] = [str(val) for val in df_test.top_schl_id.values]\n",
    "df_train['year'] = [str(val) for val in df_train.year.values]\n",
    "df_test['year'] = [str(val) for val in df_test.year.values]\n",
    "\n",
    "\n",
    "\n",
    "def input_fn(df):\n",
    "    # create a dictonary mapping from each continues features column name(k) \n",
    "    # to the values of that column stored in a constant tensor\n",
    "    continuous_cols = {k: tf.constant(df[k].values) \n",
    "                       for k in CONTINUES_COLUMNS}\n",
    "    \n",
    "    # create a dictonary mapping from each catrgorical features column name(k) \n",
    "    # to the values of that column stored in a constant tensor\n",
    "    categorical_cols = {\n",
    "        k: tf.SparseTensor(\n",
    "            indices = [[i,0] for i in range(df[k].size)],\n",
    "            values = df[k].values,\n",
    "            dense_shape = [df[k].size, 1]\n",
    "        )\n",
    "        for k in CATEGORY_COLUMNS\n",
    "    }\n",
    "    \n",
    "    # merge the two dictionaries into one\n",
    "    feature_cols = dict(continuous_cols.items() + categorical_cols.items())\n",
    "    # converts the labels column into a constant tensor\n",
    "    label = tf.constant(df[LABEL_COLUMN].values)\n",
    "    # return the feature columns and the label\n",
    "    return feature_cols, label   \n",
    "\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(df_train)\n",
    "\n",
    "def test_input_fn():\n",
    "    return input_fn(df_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2 into /var/folders/p2/4dg2st2j5dq310jd40nwwcdw0000gn/T/tmpt1CpLZ/model.ckpt.\n",
      "INFO:tensorflow:loss = 46.180183, step = 2\n",
      "INFO:tensorflow:global_step/sec: 26.5794\n",
      "INFO:tensorflow:loss = 45.33406, step = 202 (6.594 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 202 into /var/folders/p2/4dg2st2j5dq310jd40nwwcdw0000gn/T/tmpt1CpLZ/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 45.33406.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNLinearCombinedClassifier(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._BinaryLogisticHead object at 0x1c3931fd50>, 'linear_optimizer': None, 'embedding_lr_multipliers': None, 'dnn_activation_fn': <function relu at 0x10f677c80>, 'dnn_optimizer': None, 'joint_linear_weights': False, 'dnn_feature_columns': (_RealValuedColumn(column_name='par_days', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='actual_day', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='sub_tol', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='sub_avg', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='sub_avg_a', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='ex_tol', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='ex_subrate', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='sub_ins_sd', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='sub_ins_m', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='ins_m', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='auc_sub_avg', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='auc_sub_sd', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='ins_max', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='ins_summit_four_1', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='ins_summit_four_2', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='ins_summit_four_3', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='ins_summit_max', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='notscWe_sub', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='notscWe_ex', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='notscWd_sub', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='notscWd_ex', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='sc_sub', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='sc_ex', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='pub_avg', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='pub_sd', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='n_ex', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='n_pkg', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='n_pra_ex_avg', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='n_pra_ex_sd', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='delta_days', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='visa_typ_cd', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('A', 'A1', 'A2', 'A3', 'B', 'C', 'D1', 'D2', 'D3'), num_oov_buckets=0, vocab_size=9, default_value=-1), combiner='sum', dtype=tf.string), dimension=3, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x182505e090>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None, max_norm=None, trainable=True), _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='top_schl_id', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('432466', '430552', '431016', '432643', '434410', '432715', '434200', '409', '432710', '430564', '432699', '436223', '434266', '437282', '432468', '439174', '435391', '432590', '432791', '437357', '438276', '436224', '435620', '430558', '432610', '439177', '437353', '438456', '430627', '439175', '437317', '437367', '438445', '435300', '436070', '437414', '435590', '439176', '435467', '440467'), num_oov_buckets=0, vocab_size=40, default_value=-1), combiner='sum', dtype=tf.string), dimension=5, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1c3344b9d0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None, max_norm=None, trainable=True), _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='year', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('2013', '2014', '2015', '2016', '2017', '2018'), num_oov_buckets=0, vocab_size=6, default_value=-1), combiner='sum', dtype=tf.string), dimension=3, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1c3931fdd0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None, max_norm=None, trainable=True), _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='city', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('changchun', 'shanghai', 'beijin', 'zhuhai', 'yantai', 'changsha', 'chongqing', 'yangzhou', 'linfen', 'taizhou', 'wenzhou', 'wuhan', 'jiamusi', 'nantong', 'wuxi', 'hangzhou', 'dalian', 'huzhou', 'shenzheng', 'xian', 'ningbo', 'changzhou'), num_oov_buckets=0, vocab_size=22, default_value=-1), combiner='sum', dtype=tf.string), dimension=5, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1c3931fed0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None, max_norm=None, trainable=True), _EmbeddingColumn(sparse_id_column=_SparseColumn(column_name='level', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('2nd', '1st', 'new1', '4th', '3rd'), num_oov_buckets=0, vocab_size=5, default_value=-1), combiner='sum', dtype=tf.string), dimension=3, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1c33429d50>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None, max_norm=None, trainable=True)), 'input_layer_partitioner': None, 'dnn_hidden_units': [100, 50], 'dnn_dropout': None, 'gradient_clip_norm': None, 'fix_global_step_increment_bug': False, 'linear_feature_columns': (_SparseColumn(column_name='visa_typ_cd', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('A', 'A1', 'A2', 'A3', 'B', 'C', 'D1', 'D2', 'D3'), num_oov_buckets=0, vocab_size=9, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='city', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('changchun', 'shanghai', 'beijin', 'zhuhai', 'yantai', 'changsha', 'chongqing', 'yangzhou', 'linfen', 'taizhou', 'wenzhou', 'wuhan', 'jiamusi', 'nantong', 'wuxi', 'hangzhou', 'dalian', 'huzhou', 'shenzheng', 'xian', 'ningbo', 'changzhou'), num_oov_buckets=0, vocab_size=22, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='top_schl_id', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('432466', '430552', '431016', '432643', '434410', '432715', '434200', '409', '432710', '430564', '432699', '436223', '434266', '437282', '432468', '439174', '435391', '432590', '432791', '437357', '438276', '436224', '435620', '430558', '432610', '439177', '437353', '438456', '430627', '439175', '437317', '437367', '438445', '435300', '436070', '437414', '435590', '439176', '435467', '440467'), num_oov_buckets=0, vocab_size=40, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='year', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('2013', '2014', '2015', '2016', '2017', '2018'), num_oov_buckets=0, vocab_size=6, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='level', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('2nd', '1st', 'new1', '4th', '3rd'), num_oov_buckets=0, vocab_size=5, default_value=-1), combiner='sum', dtype=tf.string), _BucketizedColumn(source_column=_RealValuedColumn(column_name='par_days', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), boundaries=(7, 31, 93, 183, 365, 730)), _BucketizedColumn(source_column=_RealValuedColumn(column_name='actual_day', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), boundaries=(5, 25, 75, 150, 300, 600)), _BucketizedColumn(source_column=_RealValuedColumn(column_name='delta_days', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), boundaries=(7, 31, 93, 183, 365, 730, 1650)), _CrossedColumn(columns=(_SparseColumn(column_name='top_schl_id', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('432466', '430552', '431016', '432643', '434410', '432715', '434200', '409', '432710', '430564', '432699', '436223', '434266', '437282', '432468', '439174', '435391', '432590', '432791', '437357', '438276', '436224', '435620', '430558', '432610', '439177', '437353', '438456', '430627', '439175', '437317', '437367', '438445', '435300', '436070', '437414', '435590', '439176', '435467', '440467'), num_oov_buckets=0, vocab_size=40, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='visa_typ_cd', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('A', 'A1', 'A2', 'A3', 'B', 'C', 'D1', 'D2', 'D3'), num_oov_buckets=0, vocab_size=9, default_value=-1), combiner='sum', dtype=tf.string)), hash_bucket_size=10000, hash_key=None, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None), _CrossedColumn(columns=(_SparseColumn(column_name='visa_typ_cd', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('A', 'A1', 'A2', 'A3', 'B', 'C', 'D1', 'D2', 'D3'), num_oov_buckets=0, vocab_size=9, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='year', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('2013', '2014', '2015', '2016', '2017', '2018'), num_oov_buckets=0, vocab_size=6, default_value=-1), combiner='sum', dtype=tf.string)), hash_bucket_size=10000, hash_key=None, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None), _CrossedColumn(columns=(_SparseColumn(column_name='city', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('changchun', 'shanghai', 'beijin', 'zhuhai', 'yantai', 'changsha', 'chongqing', 'yangzhou', 'linfen', 'taizhou', 'wenzhou', 'wuhan', 'jiamusi', 'nantong', 'wuxi', 'hangzhou', 'dalian', 'huzhou', 'shenzheng', 'xian', 'ningbo', 'changzhou'), num_oov_buckets=0, vocab_size=22, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='visa_typ_cd', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('A', 'A1', 'A2', 'A3', 'B', 'C', 'D1', 'D2', 'D3'), num_oov_buckets=0, vocab_size=9, default_value=-1), combiner='sum', dtype=tf.string)), hash_bucket_size=10000, hash_key=None, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None), _CrossedColumn(columns=(_SparseColumn(column_name='level', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('2nd', '1st', 'new1', '4th', '3rd'), num_oov_buckets=0, vocab_size=5, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='visa_typ_cd', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('A', 'A1', 'A2', 'A3', 'B', 'C', 'D1', 'D2', 'D3'), num_oov_buckets=0, vocab_size=9, default_value=-1), combiner='sum', dtype=tf.string)), hash_bucket_size=10000, hash_key=None, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None), _CrossedColumn(columns=(_SparseColumn(column_name='top_schl_id', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('432466', '430552', '431016', '432643', '434410', '432715', '434200', '409', '432710', '430564', '432699', '436223', '434266', '437282', '432468', '439174', '435391', '432590', '432791', '437357', '438276', '436224', '435620', '430558', '432610', '439177', '437353', '438456', '430627', '439175', '437317', '437367', '438445', '435300', '436070', '437414', '435590', '439176', '435467', '440467'), num_oov_buckets=0, vocab_size=40, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='visa_typ_cd', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('A', 'A1', 'A2', 'A3', 'B', 'C', 'D1', 'D2', 'D3'), num_oov_buckets=0, vocab_size=9, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='year', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('2013', '2014', '2015', '2016', '2017', '2018'), num_oov_buckets=0, vocab_size=6, default_value=-1), combiner='sum', dtype=tf.string)), hash_bucket_size=10000, hash_key=None, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None), _CrossedColumn(columns=(_SparseColumn(column_name='top_schl_id', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('432466', '430552', '431016', '432643', '434410', '432715', '434200', '409', '432710', '430564', '432699', '436223', '434266', '437282', '432468', '439174', '435391', '432590', '432791', '437357', '438276', '436224', '435620', '430558', '432610', '439177', '437353', '438456', '430627', '439175', '437317', '437367', '438445', '435300', '436070', '437414', '435590', '439176', '435467', '440467'), num_oov_buckets=0, vocab_size=40, default_value=-1), combiner='sum', dtype=tf.string), _SparseColumn(column_name='year', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('2013', '2014', '2015', '2016', '2017', '2018'), num_oov_buckets=0, vocab_size=6, default_value=-1), combiner='sum', dtype=tf.string)), hash_bucket_size=10000, hash_key=None, combiner='sum', ckpt_to_load_from=None, tensor_name_in_ckpt=None))})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(input_fn = train_input_fn,steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-11-08:12:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/p2/4dg2st2j5dq310jd40nwwcdw0000gn/T/tmpt1CpLZ/model.ckpt-202\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-11-08:12:13\n",
      "INFO:tensorflow:Saving dict for global step 202: accuracy = 0.8840116, accuracy/baseline_label_mean = 0.8848837, accuracy/threshold_0.500000_mean = 0.8840116, auc = 0.49917623, auc_precision_recall = 0.93815255, global_step = 202, labels/actual_label_mean = 0.8848837, labels/prediction_mean = 0.9944089, loss = 36.40611, precision/positive_threshold_0.500000_mean = 0.88478327, recall/positive_threshold_0.500000_mean = 0.99901444\n"
     ]
    }
   ],
   "source": [
    "results = m.evaluate(input_fn=test_input_fn,steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8840116,\n",
       " 'accuracy/baseline_label_mean': 0.8848837,\n",
       " 'accuracy/threshold_0.500000_mean': 0.8840116,\n",
       " 'auc': 0.49917623,\n",
       " 'auc_precision_recall': 0.93815255,\n",
       " 'global_step': 202,\n",
       " 'labels/actual_label_mean': 0.8848837,\n",
       " 'labels/prediction_mean': 0.9944089,\n",
       " 'loss': 36.40611,\n",
       " 'precision/positive_threshold_0.500000_mean': 0.88478327,\n",
       " 'recall/positive_threshold_0.500000_mean': 0.99901444}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
